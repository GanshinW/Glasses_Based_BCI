{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc86dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== emotion_pipeline =====\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "\n",
    "from config import Config\n",
    "from utils.load_npz import load_dataset\n",
    "from utils.preprocess import preprocess_batch\n",
    "from utils.features import extract_features\n",
    "from models.multimodal_model import MultiModalNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task/ckpt\n",
    "TASK = 'emotion'\n",
    "CKPT = 'best_emotion_model.pth'\n",
    "print('TASK =', TASK, '| CKPT =', CKPT)\n",
    "\n",
    "# Load .npz\n",
    "dataset = load_dataset(task_type=TASK)\n",
    "fs = int(dataset['sampling_rate'])\n",
    "n_channels = int(dataset['X_train'].shape[1])\n",
    "\n",
    "if 'class_names' in dataset:\n",
    "    class_names = list(dataset['class_names'])\n",
    "elif 'label_map' in dataset:\n",
    "    class_names = list(dataset['label_map'].keys())\n",
    "else:\n",
    "    class_names = sorted(list(np.unique(dataset['y_train'])))\n",
    "n_classes = len(class_names)\n",
    "print('fs:', fs, '| channels:', n_channels, '| classes:', n_classes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (notch + band-pass + z-score)\n",
    "NOTCH_F0, NOTCH_Q = 50.0, 25.0\n",
    "BAND = (1.0, 40.0)\n",
    "ORDER = 4\n",
    "X_time_train = preprocess_batch(dataset['X_train'], fs, notch_freq=NOTCH_F0, notch_q=NOTCH_Q, band=BAND, order=ORDER, do_zscore=True)\n",
    "X_time_val   = preprocess_batch(dataset['X_val'],   fs, notch_freq=NOTCH_F0, notch_q=NOTCH_Q, band=BAND, order=ORDER, do_zscore=True)\n",
    "X_time_test  = preprocess_batch(dataset['X_test'],  fs, notch_freq=NOTCH_F0, notch_q=NOTCH_Q, band=BAND, order=ORDER, do_zscore=True)\n",
    "\n",
    "# Check window == 10 s (or longer)\n",
    "WIN_SEC = 10.0\n",
    "T_sec = X_time_train.shape[-1] / fs\n",
    "assert abs(T_sec - WIN_SEC) < 1e-3 or T_sec > WIN_SEC, f'Expect 10 s or longer, got {T_sec:.3f} s'\n",
    "print('Window current:', T_sec, 's (expect 10 s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69491e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw vs Preprocessed (trim overlay to match length)\n",
    "ch_names = dataset.get('channel_names', [f'ch{i+1}' for i in range(n_channels)])\n",
    "trial_idx = 0\n",
    "L_pre = min(X_time_train.shape[-1], dataset['X_train'].shape[-1])\n",
    "t_pre = np.arange(L_pre) / fs\n",
    "fig, axes = plt.subplots(nrows=n_channels, ncols=1, figsize=(10, 1.6*n_channels), sharex=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "for c in range(n_channels):\n",
    "    ax = axes[c]\n",
    "    raw = dataset['X_train'][trial_idx, c][:L_pre]\n",
    "    pre = X_time_train[trial_idx, c][:L_pre]\n",
    "    ax.plot(t_pre, raw, alpha=0.5, label='raw (trim)')\n",
    "    ax.plot(t_pre, pre, lw=1.0, label='preproc')\n",
    "    ax.set_ylabel(ch_names[c])\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "plt.suptitle('Emotion: Raw vs Preprocessed (10 s)'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfeb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD pre/post\n",
    "from scipy.signal import welch\n",
    "def psd(x, fs, nperseg=None, noverlap=None):\n",
    "    f, Pxx = welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap, scaling='density'); return f, Pxx\n",
    "chan_list = [0, min(3, n_channels-1)]\n",
    "nper = int(fs*2.0); nov = int(fs*1.0)\n",
    "for c in chan_list:\n",
    "    f_raw, P_raw = psd(dataset['X_train'][trial_idx, c], fs, nper, nov)\n",
    "    f_pre, P_pre = psd(X_time_train[trial_idx, c], fs, nper, nov)\n",
    "    m50 = (f_raw >= 48) & (f_raw <= 52)\n",
    "    p50_raw = np.trapz(P_raw[m50], f_raw[m50])\n",
    "    m50p = (f_pre >= 48) & (f_pre <= 52)\n",
    "    p50_pre = np.trapz(P_pre[m50p], f_pre[m50p])\n",
    "    print(f'Ch {ch_names[c]} 50Hz raw={p50_raw:.3e}, pre={p50_pre:.3e}, red={(1 - p50_pre/max(p50_raw,1e-12))*100:.1f}%')\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.semilogy(f_raw, P_raw, label='raw'); plt.semilogy(f_pre, P_pre, label='preproc')\n",
    "    plt.axvline(50, ls='--', lw=0.8, color='grey'); plt.xlim(0, 60)\n",
    "    plt.xlabel('Hz'); plt.ylabel('PSD'); plt.title(f'Emotion PSD pre/post — {ch_names[c]}'); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram pre/post (side-by-side, 0–40 Hz)\n",
    "from scipy.signal import spectrogram\n",
    "chan = 0\n",
    "x0 = dataset['X_train'][trial_idx, chan]; x1 = X_time_train[trial_idx, chan]\n",
    "f0, t0, S0 = spectrogram(x0, fs=fs, nperseg=int(fs*0.25), noverlap=int(fs*0.125), scaling='density', mode='psd')\n",
    "f1, t1, S1 = spectrogram(x1, fs=fs, nperseg=int(fs*0.25), noverlap=int(fs*0.125), scaling='density', mode='psd')\n",
    "def _norm_log(S): L=np.log1p(S); return (L - L.mean())/(L.std()+1e-8)\n",
    "S0n = _norm_log(S0[f0<=40]); S1n = _norm_log(S1[f1<=40])\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,3), sharey=True)\n",
    "axes[0].imshow(S0n, aspect='auto', origin='lower', extent=[t0[0], t0[-1], 0, 40]); axes[0].set_title('Raw'); axes[0].set_xlabel('s'); axes[0].set_ylabel('Hz')\n",
    "axes[1].imshow(S1n, aspect='auto', origin='lower', extent=[t1[0], t1[-1], 0, 40]); axes[1].set_title('Preproc'); axes[1].set_xlabel('s')\n",
    "plt.suptitle('Emotion: Spectrogram (0–40 Hz)'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction (time + freq + images on)\n",
    "BANDS = ((4,8),(8,13),(13,30))\n",
    "features = extract_features(\n",
    "    X_time_train, X_time_val, X_time_test, fs,\n",
    "    bands=BANDS,\n",
    "    welch_nperseg=int(fs*0.5), welch_noverlap=int(fs*0.25),\n",
    "    make_images=True,   # emotion: on\n",
    "    spec_nperseg=int(fs*0.25), spec_noverlap=int(fs*0.125),\n",
    "    fmax=40.0, channel_method='average'\n",
    ")\n",
    "for k in ['X_time_train','X_freq_train','X_img_train']:\n",
    "    v = features[k]; print(k, None if v is None else v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73315da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature QA: band-power by class + alpha/beta\n",
    "def collapse_channels(bp): return bp.mean(axis=1)\n",
    "bp_train = collapse_channels(features['X_freq_train']); y_train = dataset['y_train'].astype(int)\n",
    "bands_labels = [f'{lo}-{hi}Hz' for lo,hi in BANDS]\n",
    "classes = np.unique(y_train)\n",
    "means = np.stack([bp_train[y_train==c].mean(axis=0) for c in classes])\n",
    "stds  = np.stack([bp_train[y_train==c].std(axis=0)  for c in classes])\n",
    "x = np.arange(len(bands_labels)); width = 0.8/len(classes)\n",
    "plt.figure(figsize=(8,4))\n",
    "for i, c in enumerate(classes):\n",
    "    plt.bar(x+i*width, means[i], width=width, yerr=stds[i], capsize=3, label=str(class_names[c]))\n",
    "plt.xticks(x + width*(len(classes)-1)/2, bands_labels); plt.ylabel('log10 BP')\n",
    "plt.title('Emotion: Band-power by class'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "alpha_idx, beta_idx = 0, 2\n",
    "ratio = bp_train[:, alpha_idx] - bp_train[:, beta_idx]\n",
    "data = [ratio[y_train==c] for c in classes]\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.boxplot(data, labels=[str(class_names[c]) for c in classes], showmeans=True)\n",
    "plt.ylabel('log(alpha)-log(beta)'); plt.title('Emotion: Alpha/Beta'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "class EEGMultimodalDataset(Dataset):\n",
    "    def __init__(self, feats, split, labels):\n",
    "        self.Xt = feats[f'X_time_{split}']; self.Xf = feats[f'X_freq_{split}']; self.Xi = feats[f'X_img_{split}']\n",
    "        self.y  = labels.astype(int)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        d = {'time': torch.tensor(self.Xt[i], dtype=torch.float32),\n",
    "             'freq': torch.tensor(self.Xf[i], dtype=torch.float32),\n",
    "             'label': torch.tensor(int(self.y[i]), dtype=torch.long)}\n",
    "        if self.Xi is not None: d['img'] = torch.tensor(self.Xi[i], dtype=torch.float32)\n",
    "        return d\n",
    "\n",
    "train_loader = DataLoader(EEGMultimodalDataset(features,'train',dataset['y_train']), batch_size=Config.batch_size, shuffle=True,  num_workers=Config.num_workers)\n",
    "val_loader   = DataLoader(EEGMultimodalDataset(features,'val',  dataset['y_val']),   batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers)\n",
    "test_loader  = DataLoader(EEGMultimodalDataset(features,'test', dataset['y_test']),  batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_img = features['X_img_train'] is not None\n",
    "n_bands = int(features['X_freq_train'].shape[2])\n",
    "n_samples = int(features['X_time_train'].shape[2])\n",
    "\n",
    "model = MultiModalNet(\n",
    "    n_channels = int(features['X_time_train'].shape[1]),\n",
    "    n_samples  = n_samples,\n",
    "    n_bands    = n_bands,\n",
    "    img_out_dim= Config.img_out_dim,\n",
    "    hidden_dim = Config.hidden_dim,\n",
    "    n_classes  = n_classes,\n",
    "    use_img    = use_img\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(mode=train)\n",
    "    tot=0; ok=0; loss_sum=0.0\n",
    "    for b in loader:\n",
    "        if train: optimizer.zero_grad()\n",
    "        t = b['time'].to(device); f = b['freq'].to(device)\n",
    "        i = b.get('img'); i = i.to(device) if (use_img and i is not None) else None\n",
    "        y = b['label'].to(device)\n",
    "        logits = model(t,f,i)\n",
    "        loss = criterion(logits,y)\n",
    "        if train: loss.backward(); optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        ok += (logits.argmax(1)==y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return loss_sum/max(1,len(loader)), 100.0*ok/max(1,tot)\n",
    "\n",
    "best_val=0.0; wait=0\n",
    "hist={'train_loss':[],'val_loss':[],'train_acc':[],'val_acc':[]}\n",
    "for ep in range(1, Config.num_epochs+1):\n",
    "    tl, ta = run_epoch(train_loader, True)\n",
    "    vl, va = run_epoch(val_loader, False)\n",
    "    scheduler.step(va)\n",
    "    hist['train_loss'].append(tl); hist['val_loss'].append(vl)\n",
    "    hist['train_acc'].append(ta);  hist['val_acc'].append(va)\n",
    "    print(f'E{ep:02d} | train {ta:5.1f}%/{tl:.4f}  val {va:5.1f}%/{vl:.4f}')\n",
    "    if va > best_val:\n",
    "        best_val=va; wait=0; torch.save(model.state_dict(), CKPT)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= Config.patience:\n",
    "            print('Early stop.'); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (val + test)\n",
    "model.load_state_dict(torch.load(CKPT, map_location=device)); model.eval()\n",
    "\n",
    "def eval_split(loader):\n",
    "    yt, yp = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            t = b['time'].to(device); f = b['freq'].to(device)\n",
    "            i = b.get('img'); i = i.to(device) if (use_img and i is not None) else None\n",
    "            y = b['label'].to(device)\n",
    "            pred = model(t,f,i).argmax(1)\n",
    "            yt.extend(y.cpu().numpy()); yp.extend(pred.cpu().numpy())\n",
    "    yt, yp = np.array(yt), np.array(yp)\n",
    "    return yt, yp\n",
    "\n",
    "yt_v, yp_v = eval_split(val_loader)\n",
    "yt_t, yp_t = eval_split(test_loader)\n",
    "print(f'[VAL]  acc={accuracy_score(yt_v, yp_v):.4f}  f1-macro={f1_score(yt_v, yp_v, average=\"macro\"):.4f}')\n",
    "print(classification_report(yt_v, yp_v, target_names=[str(c) for c in class_names], digits=4))\n",
    "print(f'[TEST] acc={accuracy_score(yt_t, yp_t):.4f}  f1-macro={f1_score(yt_t, yp_t, average=\"macro\"):.4f}')\n",
    "print(classification_report(yt_t, yp_t, target_names=[str(c) for c in class_names], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b51239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curves\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(hist['train_loss']); plt.plot(hist['val_loss']); plt.title('Loss'); plt.grid(True)\n",
    "plt.subplot(1,2,2); plt.plot(hist['train_acc']);  plt.plot(hist['val_acc']);  plt.title('Accuracy'); plt.grid(True)\n",
    "plt.suptitle('Training Curves — Emotion'); plt.tight_layout(); plt.show()\n",
    "\n",
    "print('Saved ->', CKPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
